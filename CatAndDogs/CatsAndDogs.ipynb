{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b156c9ce-06db-4959-9552-3af7e4701052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T10:16:16.476454200Z",
     "start_time": "2023-05-17T10:16:13.825763100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.0.1\n",
      "Torchvision Version:  0.15.2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "import pandas as pd\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa2cb99b-89f1-4b39-9c42-8e76e66a2b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T10:16:16.478485800Z",
     "start_time": "2023-05-17T10:16:16.476454200Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"resnet\"\n",
    "means = None\n",
    "batch_size = 8\n",
    "num_epochs = 15\n",
    "#binary case\n",
    "num_classes_binary = 2\n",
    "#multi class\n",
    "num_classes_category = 37\n",
    "\n",
    "num_of_classes = num_classes_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3cd96c-677b-4700-a70f-a762df9f2c33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T10:16:16.504701Z",
     "start_time": "2023-05-17T10:16:16.481571300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2, 3, 4, 5, 9, 11, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 29, 30, 31, 32, 35, 36, 37}\n",
      "{1, 33, 34, 6, 7, 8, 10, 12, 21, 24, 27, 28}\n"
     ]
    }
   ],
   "source": [
    "filename = './data/train/oxford-iiit-pet/annotations/list.txt'\n",
    "cat_ids_set = set()\n",
    "dog_ids_set = set()\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    for line in file.readlines()[7:]:\n",
    "        split_str = line.split(' ')\n",
    "        id: int = int(split_str[1])\n",
    "        species: int = int(split_str[2])\n",
    "\n",
    "        if species == 1:\n",
    "            cat_ids_set.add(id)\n",
    "        else:\n",
    "            dog_ids_set.add(id)\n",
    "print(dog_ids_set)\n",
    "print(cat_ids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def transform_label(label):\n",
    "    return torch.flatten(torch.nn.functional.one_hot(torch.LongTensor([0 if label in cat_ids_set else 1],num_of_classes))).double()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T10:16:16.504701Z",
     "start_time": "2023-05-17T10:16:16.500280700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099e3ff5-8e9f-4547-88fd-7ce605b2e111",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T10:16:17.006158500Z",
     "start_time": "2023-05-17T10:16:16.950727Z"
    }
   },
   "outputs": [],
   "source": [
    "if num_of_classes == 2:\n",
    "    trainval_dataset = torchvision.datasets.OxfordIIITPet('./data/train/', split='trainval',  download=True,\n",
    "                                                  transform=torchvision.transforms.Compose([\n",
    "                                                      # Resize step is required as we will use a ResNet model, which accepts at leats 224x224 images\n",
    "                                                      torchvision.transforms.Resize((224,224)),\n",
    "                                                      torchvision.transforms.ToTensor(),\n",
    "                                                  ]), target_transform = torchvision.transforms.Lambda(transform_label))\n",
    "    test_dataset = torchvision.datasets.OxfordIIITPet('./data/test/', split='test',  download=True,\n",
    "                                                     transform=torchvision.transforms.Compose([\n",
    "                                                          # Resize step is required as we will use a ResNet model, which accepts at leats 224x224 images\n",
    "                                                          torchvision.transforms.Resize((224,224)),\n",
    "                                                          torchvision.transforms.ToTensor(),\n",
    "                                                      ]), target_transform = torchvision.transforms.Lambda(transform_label))\n",
    "\n",
    "else:\n",
    "    trainval_dataset = torchvision.datasets.OxfordIIITPet('./data/train/', split='trainval',  download=True,\n",
    "                                                      transform=torchvision.transforms.Compose([\n",
    "                                                          # Resize step is required as we will use a ResNet model, which accepts at leats 224x224 images\n",
    "                                                          torchvision.transforms.Resize((224,224)),\n",
    "                                                          torchvision.transforms.ToTensor(),\n",
    "                                                      ]), target_transform = torchvision.transforms.Lambda(transform_label))\n",
    "    test_dataset = torchvision.datasets.OxfordIIITPet('./data/test/', split='test',  download=True,\n",
    "                                                     transform=torchvision.transforms.Compose([\n",
    "                                                          # Resize step is required as we will use a ResNet model, which accepts at leats 224x224 images\n",
    "                                                          torchvision.transforms.Resize((224,224)),\n",
    "                                                          torchvision.transforms.ToTensor(),\n",
    "                                                      ]), target_transform = torchvision.transforms.Lambda(transform_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "if num_of_classes == 2:\n",
    "    trainval_dataset = torchvision.datasets.OxfordIIITPet('./data/train/', split='trainval',  download=True,\n",
    "                                                  transform=torchvision.transforms.Compose([\n",
    "                                                      # Resize step is required as we will use a ResNet model, which accepts at leats 224x224 images\n",
    "                                                      torchvision.transforms.Resize((224,224)),\n",
    "                                                      torchvision.transforms.ToTensor(),\n",
    "                                                  ]))\n",
    "    test_dataset = torchvision.datasets.OxfordIIITPet('./data/test/', split='test',  download=True,\n",
    "                                                     transform=torchvision.transforms.Compose([\n",
    "                                                          # Resize step is required as we will use a ResNet model, which accepts at leats 224x224 images\n",
    "                                                          torchvision.transforms.Resize((224,224)),\n",
    "                                                          torchvision.transforms.ToTensor(),\n",
    "                                                      ]))\n",
    "\n",
    "else:\n",
    "    trainval_dataset = torchvision.datasets.OxfordIIITPet('./data/train/', split='trainval',  download=True,\n",
    "                                                      transform=torchvision.transforms.Compose([\n",
    "                                                          # Resize step is required as we will use a ResNet model, which accepts at leats 224x224 images\n",
    "                                                          torchvision.transforms.Resize((224,224)),\n",
    "                                                          torchvision.transforms.ToTensor(),\n",
    "                                                      ]))\n",
    "    test_dataset = torchvision.datasets.OxfordIIITPet('./data/test/', split='test',  download=True,\n",
    "                                                     transform=torchvision.transforms.Compose([\n",
    "                                                          # Resize step is required as we will use a ResNet model, which accepts at leats 224x224 images\n",
    "                                                          torchvision.transforms.Resize((224,224)),\n",
    "                                                          torchvision.transforms.ToTensor(),\n",
    "                                                      ]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T10:06:54.578523600Z",
     "start_time": "2023-05-17T10:06:54.434286100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c1466b7-c67a-4fb5-83a5-993835d47fab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T10:16:18.755755600Z",
     "start_time": "2023-05-17T10:16:18.744734600Z"
    }
   },
   "outputs": [],
   "source": [
    "train_split_percentage: float = 0.8\n",
    "train_split: int = int(train_split_percentage * len(trainval_dataset))\n",
    "val_split: int = len(trainval_dataset) - train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff07421-5ef2-478d-b62b-3d32f7a97a1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T10:16:19.231640300Z",
     "start_time": "2023-05-17T10:16:19.223519400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, lengths=[train_split, val_split])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aa1581-11f0-42f6-a167-1751bf9e57bc",
   "metadata": {},
   "source": [
    "## Extract mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f50b323-7c1f-4865-8ef3-d9e98a7bca8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T10:16:20.359688Z",
     "start_time": "2023-05-17T10:16:20.347237300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataloader.__iter__()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-17T10:16:24.755399900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "if means is None:\n",
    "    means = []\n",
    "    stdevs = []\n",
    "    for X, _ in train_dataloader:\n",
    "        # Dimensions 0,2,3 are respectively the batch, height and width dimensions\n",
    "        means.append(X.mean(dim=(0,2,3)))\n",
    "        stdevs.append(X.std(dim=(0,2,3)))\n",
    "\n",
    "    mean = torch.stack(means, dim=0).mean(dim=0)\n",
    "    stdev = torch.stack(stdevs, dim=0).mean(dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T10:08:11.552914400Z",
     "start_time": "2023-05-17T10:07:54.345772200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean, stdev)\n",
    "])\n",
    "val_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean, stdev)\n",
    "])\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean, stdev)\n",
    "])\n",
    "\n",
    "#target_transform = torchvision.transforms.Compose([\n",
    "#    lambda x:torch.LongTensor([x]), # or just torch.tensor\n",
    "#    lambda x:torch.nn.functional.one_hot(x,num_of_classes)])\n",
    "#target_transform = torchvision.transforms.Lambda(lambda y: torch.zeros(\n",
    "#    num_of_classes, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0e0a69-22a7-4406-81c7-ab134cb21568",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.transform = train_transforms\n",
    "val_dataset.transform = val_transforms\n",
    "test_dataset.transform = test_transforms\n",
    "\n",
    "#train_dataset.target_transform = target_transform\n",
    "#val_dataset.target_transform = target_transform\n",
    "#test_dataset.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5a599-838d-44d1-9326-c4629e53e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c776d-ad50-449e-bbd5-a1c402d02011",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, lab = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f742d2-53c7-44af-a30a-dcee1569214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fce5f-a2b0-471b-9fee-df7595539a84",
   "metadata": {},
   "source": [
    "## Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab4a12-eaf0-4e54-8895-cafd0c4a35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus: int = torch.cuda.device_count()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0085e67e-18d5-4db0-8de1-631762c47d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    resnet = torchvision.models.resnet18(pretrained=True)\n",
    "    \n",
    "    # Substitute the FC output layer\n",
    "    resnet.fc = torch.nn.Linear(resnet.fc.in_features, 10)\n",
    "    torch.nn.init.xavier_uniform_(resnet.fc.weight)\n",
    "    return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2f1e2-7ef7-4c10-bab5-4099786039ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_18 = get_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319fff6f-02a7-4db7-8ef5-80b74050db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_dataloader, valid_dataloader, criterion, optimizer, scheduler=None, epochs=10, device='cpu', checkpoint_epochs=10):\n",
    "    start = time.time()\n",
    "    print(f'Training for {epochs} epochs on {device}')\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        \n",
    "        net.train()  # put network in train mode for Dropout and Batch Normalization\n",
    "        train_loss = torch.tensor(0., device=device)  # loss and accuracy tensors are on the GPU to avoid data transfers\n",
    "        train_accuracy = torch.tensor(0., device=device)\n",
    "        for X, y in train_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            preds = net(X)\n",
    "            loss = criterion(preds, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                train_loss += loss * train_dataloader.batch_size\n",
    "                train_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n",
    "        \n",
    "        if valid_dataloader is not None:\n",
    "            net.eval()  # put network in train mode for Dropout and Batch Normalization\n",
    "            valid_loss = torch.tensor(0., device=device)\n",
    "            valid_accuracy = torch.tensor(0., device=device)\n",
    "            with torch.no_grad():\n",
    "                for X, y in valid_dataloader:\n",
    "                    X = X.to(device)\n",
    "                    y = y.to(device)\n",
    "                    preds = net(X)\n",
    "                    loss = criterion(preds, y)\n",
    "\n",
    "                    valid_loss += loss * valid_dataloader.batch_size\n",
    "                    valid_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n",
    "        \n",
    "        if scheduler is not None: \n",
    "            scheduler.step()\n",
    "            \n",
    "        print(f'Training loss: {train_loss/len(train_dataloader.dataset):.2f}')\n",
    "        print(f'Training accuracy: {100*train_accuracy/len(train_dataloader.dataset):.2f}')\n",
    "        \n",
    "        if valid_dataloader is not None:\n",
    "            print(f'Valid loss: {valid_loss/len(valid_dataloader.dataset):.2f}')\n",
    "            print(f'Valid accuracy: {100*valid_accuracy/len(valid_dataloader.dataset):.2f}')\n",
    "        \n",
    "        if epoch%checkpoint_epochs==0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, './checkpoint.pth.tar')\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Total training time: {end-start:.1f} seconds')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eefd17-18eb-494a-9c80-9ea753f68087",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_18.to(device)\n",
    "\n",
    "# Standard CrossEntropy Loss for multi-class classification problems\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "params_1x = [param for name, param in resnet_18.named_parameters() if 'fc' not in str(name)]\n",
    "\n",
    "optimizer = torch.optim.Adam(params_1x, lr=0.0001)\n",
    "\n",
    "resnet_18_trained = train(resnet_18,\n",
    "                        train_dataloader,\n",
    "                        val_dataloader,\n",
    "                        criterion,\n",
    "                        optimizer,\n",
    "                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b56573-59a1-4ed7-9563-b27098ac4e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b67366-ff56-474b-a0e2-e6cd2e6c2206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
